# 现阶段仅仅爬取某电影的200条电影短评
# 接下来的任务是加入一些登录信息的代码，爬取某部电影的全部短评



# 下面是分批次记录每一个过程
# 第一轮：

##第一轮（爬到一个网址的具体内容）
import requests
from requests.exceptions import RequestException

def get_one_page(url):
    try:
        response = requests.get(url)
    # 判断请求结果，（状态码）
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None

def main():
    url = 'https://maoyan.com/board/4'
    html = get_one_page(url)
    print(html)

if __name__ == '__main__' :
    main()
    
##第二轮（解析）
import requests
from requests.exceptions import RequestException
import re

def get_one_page(url):
    try:
        response = requests.get(url)
    # 判断请求结果，（状态码）
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None
        
def parse_one_page(html):
    pattern = re.compile(r'<i.*?board-index.*?">(\d+)</i>'
                            '.*?data-src="(.*?)"'
                            '.*?<p class="name"><a'+'.*?title=.*?>(.*?)</a>'
                            '.*?star">(.*?)</p>'
                            '.*?class="releasetime">(.*?)</p>'
                            '.*?class="integer">(.*?)</i>'
                            .*?class="fraction">(.*?)</i>',re.S)
                            # <i  class = "+++board-index+++>1</i>，
                            # 已经重新修改过了，可以使用^^
                            # re.S表示是匹配任意的符号
   
     items = re.findall(pattern,html) 
                                    #(匹配的正则， 匹配的文本)
     print(items)
     #[( ),( ),( )] 
 
##第三轮（重新整理，将数据放到字典中）
import requests
from requests.exceptions import RequestException
import re

def get_one_page(url):
    try:
        response = requests.get(url)
    # 判断请求结果，（状态码）
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None
  
def parse_one_page(html):
    pattern = re.compile(r'<i.*?board-index.*?">(\d+)</i>'
                            '.*?data-src="(.*?)"'
                            '.*?<p class="name"><a'+'.*?title=.*?>(.*?)</a>'
                            '.*?star">(.*?)</p>'
                            '.*?class="releasetime">(.*?)</p>'
                            '.*?class="integer">(.*?)</i>'
                            .*?class="fraction">(.*?)</i>',re.S)  
                            
    items = re.findall(pattern,html) 
                                    #(匹配的正则， 匹配的文本)
    for i in items:
        yield  {
            'index' : i[0],
            'image': i[1],
            'title' : i[2],
            'actor' : i[3].strip()[3:],
            'time' : i[4].strip()[5:],
            'score' : i[5] +i[6]          
        }
 def main():
    url = 'https://maoyan.com/board/4'
    html = get_one_page(url)
    for item in parse_one_page(html):
        print(item)
    
if __name__ == '__main__' :
    main()     

##第四轮（将整理的数据以字典的形式写入文件）
import requests
from requests.exceptions import RequestException
import re
import json

def get_one_page(url):
    try:
        response = requests.get(url)
    # 判断请求结果，（状态码）
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None
        
def write_to_file(content):
    with open('E:/user/resuly.txt','a') as f:
        f.write(json.dumps(content) +'\n')
        
def  parse_one_page(html):
    pattern = re.compile(r'<i.*?board-index.*?">(\d+)</i>'
                          '.*?data-src="(.*?)"'
                          '.*?<p class="name"><a'+'.*?title=.*?>(.*?)</a>'
                          '.*?star">(.*?)</p>'
                          '.*?class="releasetime">(.*?)</p>'
                          '.*?class="integer">(.*?)</i>'
                          '.*?class="fraction">(.*?)</i>',re.S)
    
    items = re.findall(pattern,html) 
                                    #(匹配的正则， 匹配的文本)
    for i in items:
        yield  {
            'index' : i[0],
            'image': i[1],
            'title' : i[2],
            'actor' : i[3].strip()[3:],
            'time' : i[4].strip()[5:],
            'score' : i[5] +i[6]          
        }

def main():
    url = 'https://maoyan.com/board/4'
    html = get_one_page(url)
    for item in parse_one_page(html):
        print(item)
        write_to_file(item)

if __name__ == '__main__' :
    main()

##第五轮 （对写入的文件进行解码）
import requests
from requests.exceptions import RequestException
import re
import json

def get_one_page(url):
    try:
        response = requests.get(url)
    # 判断请求结果，（状态码）
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None

def write_to_file(content):
    with open('E:/user/resuly.txt','a',encoding='utf-8') as f:
        f.write(json.dumps(content,ensure_ascii = False) +'\n')

def parse_one_page(html):
    pattern = re.compile(r'<i.*?board-index.*?">(\d+)</i>'
                            '.*?data-src="(.*?)"'
                            '.*?<p class="name"><a'+'.*?title=.*?>(.*?)</a>'
                            '.*?star">(.*?)</p>'
                            '.*?class="releasetime">(.*?)</p>'
                            '.*?class="integer">(.*?)</i>'
                            '.*?class="fraction">(.*?)</i>',re.S)
    
    items = re.findall(pattern,html) 
                                    #(匹配的正则， 匹配的文本)
    for i in items:
        yield  {
            'index' : i[0],
            'image': i[1],
            'title' : i[2],
            'actor' : i[3].strip()[3:],
            'time' : i[4].strip()[5:],
            'score' : i[5] +i[6]          
        }

def main():
    url = 'https://maoyan.com/board/4'
    html = get_one_page(url)
    for item in parse_one_page(html):
        print(item)
        write_to_file(item)

if __name__ == '__main__' :
    main()   
    
##第六轮（广义化）
import requests
from requests.exceptions import RequestException
import re
import json

def get_one_page(url):
    try:
        response = requests.get(url)
    # 判断请求结果，（状态码）
        if response.status_code == 200:
            return response.text
        return None
    except RequestException:
        return None

def write_to_file(content):
    with open('E:/user/resuly.txt','a',encoding='utf-8') as f:
        f.write(json.dumps(content,ensure_ascii = False) +'\n')

def parse_one_page(html):
    pattern = re.compile(r'<i.*?board-index.*?">(\d+)</i>'
                          '.*?data-src="(.*?)"'
                          '.*?<p class="name"><a'+'.*?title=.*?>(.*?)</a>'
                          '.*?star">(.*?)</p>'
                          '.*?class="releasetime">(.*?)</p>'
                          '.*?class="integer">(.*?)</i>'
                          '.*?class="fraction">(.*?)</i>',re.S)
    
    items = re.findall(pattern,html) 
                                    #(匹配的正则， 匹配的文本)
    for i in items:
        yield  {
            'index' : i[0],
            'image': i[1],
            'title' : i[2],
            'actor' : i[3].strip()[3:],
            'time' : i[4].strip()[5:],
            'score' : i[5] +i[6]          
        } 
  
def main(offset):
    url = 'https://maoyan.com/board/4?offset='+ str(offset)
    html = get_one_page(url)
    for item in parse_one_page(html):
        print(item)
        write_to_file(item)

if __name__ == '__main__' :
    for i in range(10):
        main(10*i)
